{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SacasmDetect.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hoangnhan12-arc/CS114.K21.KHTN/blob/master/SacasmDetect.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCEYNsDroC-S",
        "colab_type": "text"
      },
      "source": [
        "#            SARCASM DETECTION\n",
        "NHÓM:\n",
        "1. THÁI HOÀNG NHÂN - 1852\n",
        "2. NGUYỄN VƯƠNG THỊNH - 18520367\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpPrzQXAKSkR",
        "colab_type": "text"
      },
      "source": [
        "# 1. MÔ TẢ BÀI TOÁN VÀ CÁCH THU THẬP DỮ LIỆU:\n",
        "- MÔ TẢ BÀI TOÁN\n",
        "\n",
        "  Do **Sarcasm Detection**(phát hiện châm biếm) từng thu thập dataset từ **Twitter** trên **hashtag**. Do có tweets liên quan với nhau và có nhiều bộ data gây nhiễu. Để khắc phục vấn đề, người ta dùng dataset được thu thập từ 2 trang web **TheOnion** và **HuffPost**. Data từ **TheOnion** thì hoàn toàn là châm biếm còn Data từ **HuffPost** thì là chính thống, không châm biếm. Các data được thu thập là các **headlines**(tiêu đề) của các bài báo\n",
        "\n",
        "  **Ưu điểm:**\n",
        " - Vì là các bài báo nên ngôn từ sẽ chuẩn mực, ít bị sai chính tả, ngữ pháp\n",
        " - Do các headlines trong các bài báo sẽ độc lập với nhau không như các tweets liên quan với nhau\n",
        " - Do 2 web thường xuyên ra những bài báo mới nên sẽ có bộ data chất lượng hơn, ít gây nhiễu hơn\n",
        "\n",
        "- CÁCH THU THẬP DỮ LIỆU:\n",
        " - Crawl data từ 2 trang web **HuffPost** và **TheOnion** chứa ở 2 file json riêng\n",
        " - Gộp 2 file json lại và tiến hành \"xáo trộn\" lại dữ liệu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPIGQtOAM55D",
        "colab_type": "text"
      },
      "source": [
        "#2. THU THẬP HƠN 2000 HEADLINES"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53ZB_9FJCSEt",
        "colab_type": "text"
      },
      "source": [
        "# CRAWL DATA\n",
        "\n",
        "- Công cụ: Scrapy\n",
        "- Nguồn crawl:\n",
        "  - https://www.theonion.com/ \n",
        "  - https://www.huffpost.com/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aR3qLBHqDSgn",
        "colab_type": "text"
      },
      "source": [
        "# SETTING NOTEBOOK- INSTALLING SCRAPY"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPAbM3H9Ry1n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Settings for notebook\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X81Vq-FlR-Sd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "outputId": "35a2e64c-bda2-4117-a660-4a93025e268c"
      },
      "source": [
        "!pip install scrapy\n",
        "import scrapy\n",
        "from scrapy.crawler import CrawlerProcess"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scrapy in /usr/local/lib/python3.6/dist-packages (2.2.0)\n",
            "Requirement already satisfied: lxml>=3.5.0 in /usr/local/lib/python3.6/dist-packages (from scrapy) (4.2.6)\n",
            "Requirement already satisfied: PyDispatcher>=2.0.5 in /usr/local/lib/python3.6/dist-packages (from scrapy) (2.0.5)\n",
            "Requirement already satisfied: cssselect>=0.9.1 in /usr/local/lib/python3.6/dist-packages (from scrapy) (1.1.0)\n",
            "Requirement already satisfied: Twisted>=17.9.0 in /usr/local/lib/python3.6/dist-packages (from scrapy) (20.3.0)\n",
            "Requirement already satisfied: w3lib>=1.17.0 in /usr/local/lib/python3.6/dist-packages (from scrapy) (1.22.0)\n",
            "Requirement already satisfied: zope.interface>=4.1.3 in /usr/local/lib/python3.6/dist-packages (from scrapy) (5.1.0)\n",
            "Requirement already satisfied: queuelib>=1.4.2 in /usr/local/lib/python3.6/dist-packages (from scrapy) (1.5.0)\n",
            "Requirement already satisfied: cryptography>=2.0 in /usr/local/lib/python3.6/dist-packages (from scrapy) (2.9.2)\n",
            "Requirement already satisfied: protego>=0.1.15 in /usr/local/lib/python3.6/dist-packages (from scrapy) (0.1.16)\n",
            "Requirement already satisfied: itemadapter>=0.1.0 in /usr/local/lib/python3.6/dist-packages (from scrapy) (0.1.0)\n",
            "Requirement already satisfied: service-identity>=16.0.0 in /usr/local/lib/python3.6/dist-packages (from scrapy) (18.1.0)\n",
            "Requirement already satisfied: parsel>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from scrapy) (1.6.0)\n",
            "Requirement already satisfied: pyOpenSSL>=16.2.0 in /usr/local/lib/python3.6/dist-packages (from scrapy) (19.1.0)\n",
            "Requirement already satisfied: hyperlink>=17.1.1 in /usr/local/lib/python3.6/dist-packages (from Twisted>=17.9.0->scrapy) (19.0.0)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.6/dist-packages (from Twisted>=17.9.0->scrapy) (19.3.0)\n",
            "Requirement already satisfied: PyHamcrest!=1.10.0,>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Twisted>=17.9.0->scrapy) (2.0.2)\n",
            "Requirement already satisfied: Automat>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from Twisted>=17.9.0->scrapy) (20.2.0)\n",
            "Requirement already satisfied: incremental>=16.10.1 in /usr/local/lib/python3.6/dist-packages (from Twisted>=17.9.0->scrapy) (17.5.0)\n",
            "Requirement already satisfied: constantly>=15.1 in /usr/local/lib/python3.6/dist-packages (from Twisted>=17.9.0->scrapy) (15.1.0)\n",
            "Requirement already satisfied: six>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from w3lib>=1.17.0->scrapy) (1.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from zope.interface>=4.1.3->scrapy) (47.3.1)\n",
            "Requirement already satisfied: cffi!=1.11.3,>=1.8 in /usr/local/lib/python3.6/dist-packages (from cryptography>=2.0->scrapy) (1.14.0)\n",
            "Requirement already satisfied: pyasn1 in /usr/local/lib/python3.6/dist-packages (from service-identity>=16.0.0->scrapy) (0.4.8)\n",
            "Requirement already satisfied: pyasn1-modules in /usr/local/lib/python3.6/dist-packages (from service-identity>=16.0.0->scrapy) (0.2.8)\n",
            "Requirement already satisfied: idna>=2.5 in /usr/local/lib/python3.6/dist-packages (from hyperlink>=17.1.1->Twisted>=17.9.0->scrapy) (2.9)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.0->scrapy) (2.20)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVqk7-JqUBlP",
        "colab_type": "text"
      },
      "source": [
        "# CRAWL \"https://www.theonion.com/\"\n",
        "\n",
        "1. Class ghi data vào file json"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEPgujvxSJ8o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "\n",
        "class JsonWriterPipeline(object):\n",
        "\n",
        "    def open_spider(self, spider):\n",
        "        self.file = open('theonion.jl', 'w')\n",
        "\n",
        "    def close_spider(self, spider):\n",
        "        self.file.close()\n",
        "\n",
        "    def process_item(self, item, spider):\n",
        "        line = json.dumps(dict(item)) + \"\\n\"\n",
        "        self.file.write(line)\n",
        "        return item\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4DgG7I-sEZV",
        "colab_type": "text"
      },
      "source": [
        "2. Class crawl data từ \"https://www.theonion.com/\"\n",
        "- Do web có phần chuyển sang trang kế tiếp nên chúng em crawl trong phần **Elements** trong **Inspect** và theo đường link dẫn tới trang kế tiếp để crawl\n",
        "- Các mục chúng em crawl trong web là:\n",
        " - policitcs  \"https://politics.theonion.com/\"\n",
        " - entertainment \"https://entertainment.theonion.com/\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBhtBV96SLZR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import logging\n",
        "class TheOnionSpider(scrapy.Spider):\n",
        "    name = \"TheOnion\"\n",
        "    start_urls = [\n",
        "      \"https://politics.theonion.com/\",\n",
        "      \"https://entertainment.theonion.com/\"\n",
        "    ]\n",
        "    # Tạo luồng pipelines\n",
        "    custom_settings = {\n",
        "        'LOG_LEVEL': logging.WARNING,\n",
        "        'ITEM_PIPELINES': {'__main__.JsonWriterPipeline': 1},\n",
        "        'FEED_FORMAT':'json',                                \n",
        "        'FEED_URI': 'theonion.json'                        \n",
        "    }\n",
        "    \n",
        "    def parse(self, response):\n",
        "        flag = 1\n",
        "        for article in response.css('article.cw4lnv-0.iTueKC.js_post_item'):\n",
        "            # Lọc các article có năm phát hành từ 2016 đến nay\n",
        "            if article.css('div.sc-3nbvzd-1.kpXIm::text').extract_first()[-2:] < '16' :\n",
        "              flag = 0\n",
        "              break\n",
        "              # Lọc các article có sẵn link\n",
        "            elif article.css('figure.sc-1xh12qx-0.vvKzB.cw4lnv-1.fKLmwI a::attr(href)').extract_first() is not None:\n",
        "              yield {\n",
        "                 #Trích dẫn các đường link và tiêu đề \n",
        "                'article_link': article.css('figure.sc-1xh12qx-0.vvKzB.cw4lnv-1.fKLmwI a::attr(href)').extract_first(),\n",
        "                'headline': article.css('h2.sc-759qgu-0.cYlVdn.cw4lnv-6.eXwNRE::text').extract_first(),\n",
        "                'is_sarcastic': 1\n",
        "            }\n",
        "        # Crawl theo đường dẫn qua trang khác\n",
        "        if flag == 1:\n",
        "          next_page = response.css('div.sc-1uzyw0z-0.kNHeFZ a::attr(href)').get()\n",
        "          if next_page is not None:\n",
        "              next_page = response.urljoin(next_page)\n",
        "              yield scrapy.Request(next_page, callback=self.parse)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bW7hAlgbsSep",
        "colab_type": "text"
      },
      "source": [
        "3. Chạy Crawler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5Fin5F2SQwL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "outputId": "f5b9a8ff-bb06-4667-bf58-e5def99dbbc3"
      },
      "source": [
        "process = CrawlerProcess()\n",
        "process.crawl(TheOnionSpider)\n",
        "process.start()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-06-25 14:34:13 [scrapy.utils.log] INFO: Scrapy 2.2.0 started (bot: scrapybot)\n",
            "2020-06-25 14:34:13 [scrapy.utils.log] INFO: Versions: lxml 4.2.6.0, libxml2 2.9.8, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 20.3.0, Python 3.6.9 (default, Apr 18 2020, 01:56:04) - [GCC 8.4.0], pyOpenSSL 19.1.0 (OpenSSL 1.1.1g  21 Apr 2020), cryptography 2.9.2, Platform Linux-4.19.104+-x86_64-with-Ubuntu-18.04-bionic\n",
            "2020-06-25 14:34:13 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.epollreactor.EPollReactor\n",
            "2020-06-25 14:34:13 [scrapy.crawler] INFO: Overridden settings:\n",
            "{'LOG_LEVEL': 30}\n",
            "/usr/local/lib/python3.6/dist-packages/scrapy/extensions/feedexport.py:210: ScrapyDeprecationWarning: The `FEED_URI` and `FEED_FORMAT` settings have been deprecated in favor of the `FEEDS` setting. Please see the `FEEDS` setting docs for more details\n",
            "  exporter = cls(crawler)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Deferred at 0x7fbec5aa4390>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVA2T_kBsaFc",
        "colab_type": "text"
      },
      "source": [
        "4. Xuất file json vừa ghi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGv5X7OPSZHG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "outputId": "029b98e7-260e-4870-f870-380a9ed6e548"
      },
      "source": [
        "import pandas as pd\n",
        "dfjson = pd.read_json('theonion.json')\n",
        "print(dfjson)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                           article_link  ... is_sarcastic\n",
            "0     https://entertainment.theonion.com/tuba-player...  ...            1\n",
            "1     https://entertainment.theonion.com/your-horosc...  ...            1\n",
            "2     https://entertainment.theonion.com/your-horosc...  ...            1\n",
            "3     https://entertainment.theonion.com/what-to-rea...  ...            1\n",
            "4     https://entertainment.theonion.com/your-horosc...  ...            1\n",
            "...                                                 ...  ...          ...\n",
            "1145  https://politics.theonion.com/revelations-from...  ...            1\n",
            "1146  https://politics.theonion.com/whos-speaking-at...  ...            1\n",
            "1147  https://politics.theonion.com/whos-speaking-at...  ...            1\n",
            "1148  https://politics.theonion.com/whos-speaking-at...  ...            1\n",
            "1149  https://politics.theonion.com/whos-speaking-at...  ...            1\n",
            "\n",
            "[1150 rows x 3 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAj0hkI6cXBl",
        "colab_type": "text"
      },
      "source": [
        "# CRAWL \"https://www.huffpost.com/\"\n",
        "1. Class ghi data vào file json"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOMG0sQEzsqm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "\n",
        "class JsonWriterPipeline(object):\n",
        "\n",
        "    def open_spider(self, spider):\n",
        "        self.file = open('huffpost.jl', 'w')\n",
        "\n",
        "    def close_spider(self, spider):\n",
        "        self.file.close()\n",
        "\n",
        "    def process_item(self, item, spider):\n",
        "        line = json.dumps(dict(item)) + \"\\n\"\n",
        "        self.file.write(line)\n",
        "        return item\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nTIm_rutrGo",
        "colab_type": "text"
      },
      "source": [
        "2. Class crawl data từ \"https://www.huffpost.com/\"\n",
        "- Do web chỉ có nút \"load more\" nên chúng em crawl trong phần **Network** trong **Inspect**. \n",
        "- Khi bấm vào nút \"load more\" sẽ có đường dẫn 'https://www.huffpost.com/api/department/news/cards?page=2&limit=10' hiện trong **Network**. Tụi em sẽ lấy đường dẫn này để crawl các trang tiếp theo bằng cách gán số vào \"page=\" trong đường dẫn  \n",
        "- Các mục chúng em crawl trong web là:\n",
        " - policitcs  \"https://www.huffpost.com/api/section/politics/cards?page=2&limit=10\"\n",
        " - entertainment \"https://www.huffpost.com/api/department/entertainment/cards?page=2&limit=10\"\n",
        " - work/life và wellness trong life là \"https://www.huffpost.com/api/section/worklife/cards?page=2&limit=10\" và \"https://www.huffpost.com/api/section/healthy-living/cards?page=2&limit=10\" \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDzWrF7azvco",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import logging\n",
        "class HuffPostSpider(scrapy.Spider):\n",
        "    name = \"HuffPost\"\n",
        "    base_url = 'https://www.huffpost.com/api/department/news/cards?page=%s&limit=10'\n",
        "    start_urls = [base_url % 1]\n",
        "    \n",
        "    #Tạo luồng pipelines    \n",
        "    custom_settings = {\n",
        "        'LOG_LEVEL': logging.WARNING,\n",
        "        'ITEM_PIPELINES': {'__main__.JsonWriterPipeline': 1}, \n",
        "        'FEED_FORMAT':'json',                                 \n",
        "        'FEED_URI': 'huffpost.json'                        \n",
        "    }\n",
        "    def parse(self, response):\n",
        "\n",
        "        Links = ['https://www.huffpost.com/api/section/politics/cards?page=%s&limit=10', \n",
        "                'https://www.huffpost.com/api/department/entertainment/cards?page=%s&limit=10',\n",
        "                'https://www.huffpost.com/api/section/worklife/cards?page=%s&limit=10',\n",
        "                'https://www.huffpost.com/api/section/healthy-living/cards?page=%s&limit=10']\n",
        "        # Crawl theo đường dẫn\n",
        "        for link in Links:\n",
        "             for page in range(1,200):\n",
        "                yield scrapy.Request(link % page, callback = self.parseNextPage)\n",
        "\n",
        "    def parseNextPage(self, response):\n",
        "\n",
        "      data = json.loads(response.body)\n",
        "      for article in data['cards']:\n",
        "            \n",
        "            yield {\n",
        "                #Trích dẫn các đường link và tiêu đề\n",
        "               'article_link': article['headlines'][0]['url'] ,\n",
        "               'headline': article['headlines'][0]['text'], \n",
        "               'is_sarcastic': 0 \n",
        "            }\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-w3NM1Yxrik",
        "colab_type": "text"
      },
      "source": [
        "3. Chạy crawler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-UQin_l054U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        },
        "outputId": "e0eac88e-ee6c-406d-eb60-a8d547dbcf2f"
      },
      "source": [
        "process = CrawlerProcess({'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36'})\n",
        "process.crawl(HuffPostSpider)\n",
        "process.start()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-06-25 14:58:21 [scrapy.utils.log] INFO: Scrapy 2.2.0 started (bot: scrapybot)\n",
            "2020-06-25 14:58:21 [scrapy.utils.log] INFO: Versions: lxml 4.2.6.0, libxml2 2.9.8, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 20.3.0, Python 3.6.9 (default, Apr 18 2020, 01:56:04) - [GCC 8.4.0], pyOpenSSL 19.1.0 (OpenSSL 1.1.1g  21 Apr 2020), cryptography 2.9.2, Platform Linux-4.19.104+-x86_64-with-Ubuntu-18.04-bionic\n",
            "2020-06-25 14:58:21 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.epollreactor.EPollReactor\n",
            "2020-06-25 14:58:21 [scrapy.crawler] INFO: Overridden settings:\n",
            "{'LOG_LEVEL': 30,\n",
            " 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '\n",
            "               '(KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36'}\n",
            "/usr/local/lib/python3.6/dist-packages/scrapy/extensions/feedexport.py:210: ScrapyDeprecationWarning: The `FEED_URI` and `FEED_FORMAT` settings have been deprecated in favor of the `FEEDS` setting. Please see the `FEEDS` setting docs for more details\n",
            "  exporter = cls(crawler)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9E7fEJL5xu78",
        "colab_type": "text"
      },
      "source": [
        "4. Xuất file json vừa ghi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USbhY0PY09n6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "outputId": "cffb4e73-bcc4-420b-eb8c-69b2e2c96e80"
      },
      "source": [
        "import pandas as pd\n",
        "dfjson = pd.read_json('huffpost.json')\n",
        "print(dfjson)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                           article_link  ... is_sarcastic\n",
            "0     https://www.huffpost.com/entry/aclu-john-bolto...  ...            0\n",
            "1     https://www.huffpost.com/entry/lara-trump-dona...  ...            0\n",
            "2     https://www.huffpost.com/entry/don-winslow-don...  ...            0\n",
            "3     https://www.huffpost.com/entry/us-needs-repara...  ...            0\n",
            "4     https://www.huffpost.com/entry/william-cohen-d...  ...            0\n",
            "...                                                 ...  ...          ...\n",
            "1353  https://www.huffpost.com/entry/andrew-yang-don...  ...            0\n",
            "1354  https://www.huffpost.com/entry/fox-news-tucker...  ...            0\n",
            "1355  https://www.huffpost.com/entry/coronavirus-cas...  ...            0\n",
            "1356  https://www.huffpost.com/entry/oakland-schools...  ...            0\n",
            "1357  https://www.huffpost.com/entry/william-barr-ju...  ...            0\n",
            "\n",
            "[1358 rows x 3 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67N9JL_lp8Ko",
        "colab_type": "text"
      },
      "source": [
        "# MERGE FILES.JSON AND SHUFFLE FILE.JSON"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1h7XZMvnmu3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import random \n",
        "  \n",
        "with open('theonion.json','r') as json_file:\n",
        "    theonion = json.load(json_file)\n",
        "\n",
        "with open('huffpost.json','r') as json_file:\n",
        "    huffpost = json.load(json_file)\n",
        "\n",
        "Sarcasm_Headlines_Dataset= theonion + huffpost\n",
        "random.shuffle(Sarcasm_Headlines_Dataset)\n",
        "\n",
        "with open('Sarcasm_Headlines_Dataset_v2.json', 'w') as json_file:\n",
        "    json.dump(Sarcasm_Headlines_Dataset, json_file)\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7h5BzGrn9SU",
        "colab_type": "text"
      },
      "source": [
        "# TRAINNING MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KMXSIWAkPhl",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "795bb118-7b38-47d0-b696-cfb280880144"
      },
      "source": [
        "from google.colab import files\n",
        "file = files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-c328fca7-fb6e-42eb-a4aa-be4b7df03f18\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-c328fca7-fb6e-42eb-a4aa-be4b7df03f18\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving Sarcasm_Headlines_Dataset.json to Sarcasm_Headlines_Dataset.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZIcaLMERTSb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "import pandas as pd \n",
        "\n",
        "data = pd.read_json(\"Sarcasm_Headlines_Dataset.json\",lines=True)\n",
        "\n",
        "X_train = data['headline'][0:20000]\n",
        "Y_train = data['is_sarcastic'][0:20000]\n",
        "XX_test = data['headline'][20001:]\n",
        "YY_test = data['is_sarcastic'][20001:]\n",
        "\n",
        "X_train = X_train.values\n",
        "XX_test = XX_test.values"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8SbdOozmXCI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "9f6e5d2b-4fd7-4958-a748-59269fa2f0af"
      },
      "source": [
        "print(X_train.shape)\n",
        "print(XX_test.shape)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(20000,)\n",
            "(6708,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ec8zALdgvF7K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#creat train countvecotrize \n",
        "cv_train = CountVectorizer()\n",
        "\n",
        "X_train = cv_train.fit_transform(X_train)\n",
        "#convert to array\n",
        "X_train = X_train.toarray()\n",
        " \n",
        "#creat test countvecotrize \n",
        "cv_test = CountVectorizer(vocabulary=cv_train.get_feature_names())\n",
        "\n",
        "#convert to array\n",
        "XX_test = cv_test.fit_transform(XX_test).toarray()"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IdqbbOJknt_g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test = pd.read_json(\"Sarcasm_Headlines_Dataset_v2.json\")"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfPsb1HLcDbf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0cb71eac-bdf0-4b40-ffb3-a87094a0bcdd"
      },
      "source": [
        "print(type(test['headline']))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.series.Series'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A95ddVCToDYF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_test = test['headline']\n",
        "X_test=X_test.values\n",
        "Y_test = test['is_sarcastic']"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDN-DK9WmSzV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#convert to array\n",
        "X_test = cv_test.fit_transform(X_test).toarray()"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQblYJjtRzzo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "26349a8c-9c54-4343-b882-596453e43f35"
      },
      "source": [
        "model = LogisticRegression()\n",
        "\n",
        "#training\n",
        "model.fit(X_train, Y_train)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fheoqinAR2Xf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#predict\n",
        "predictions1_1 = model.predict(XX_test)\n",
        "predictions2 = model.predict(X_test)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYWIs0j4h1Kg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "40958156-7cc2-429a-a082-81a64df51476"
      },
      "source": [
        "#traing\n",
        "model = LinearSVC()\n",
        "model.fit(X_train, Y_train)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
              "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
              "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
              "          verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPe3YfdBh2PH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#predict\n",
        "predictions1_2 = model.predict(XX_test)\n",
        "predictions3 = model.predict(X_test)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_U3SEZ5lvIv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 622
        },
        "outputId": "6a32a47a-80b3-4ceb-a159-25d95f888dfb"
      },
      "source": [
        "print(classification_report(YY_test,predictions1_1))\n",
        "#using LinearRegression with old test_data\n",
        "print(classification_report(Y_test,predictions2))\n",
        "#using LinearRegression with new test_data\n",
        "print(classification_report(YY_test,predictions1_2))\n",
        "#using LinearSVC with old test_data\n",
        "print(classification_report(Y_test,predictions3))\n",
        "#using LinearSVC with new test_data"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.87      0.86      3779\n",
            "           1       0.83      0.79      0.81      2929\n",
            "\n",
            "    accuracy                           0.84      6708\n",
            "   macro avg       0.84      0.83      0.83      6708\n",
            "weighted avg       0.84      0.84      0.84      6708\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.68      0.88      0.77      1358\n",
            "           1       0.79      0.51      0.62      1150\n",
            "\n",
            "    accuracy                           0.71      2508\n",
            "   macro avg       0.73      0.70      0.69      2508\n",
            "weighted avg       0.73      0.71      0.70      2508\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.86      0.84      3779\n",
            "           1       0.81      0.76      0.79      2929\n",
            "\n",
            "    accuracy                           0.82      6708\n",
            "   macro avg       0.82      0.81      0.81      6708\n",
            "weighted avg       0.82      0.82      0.82      6708\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.68      0.86      0.76      1358\n",
            "           1       0.76      0.53      0.62      1150\n",
            "\n",
            "    accuracy                           0.71      2508\n",
            "   macro avg       0.72      0.69      0.69      2508\n",
            "weighted avg       0.72      0.71      0.70      2508\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cegy9c86hP4e",
        "colab_type": "text"
      },
      "source": [
        "PREDICT ONE INPUT STRING\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O14mD6NtmL75",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "95c99c2c-0c0c-430d-ddc9-eb793daa637b"
      },
      "source": [
        "import numpy as np\n",
        "n = input()\n",
        "n = [n]\n",
        "\n",
        "n = np.asarray(n)\n",
        "\n",
        "\n",
        "n = cv_test.fit_transform(n).toarray()\n",
        "predict = model.predict(n)\n",
        "if predict==1:\n",
        "  print(\"=> Is sarcastic\")\n",
        "else:\n",
        "  print(\"=> Isn't sarcastic\")\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Donald Trump is stupid\n",
            "=> Isn't sarcastic\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5dnxdkzwVGC",
        "colab_type": "text"
      },
      "source": [
        "1+2)\n",
        "\n",
        "Đã mô tả trong file ipynb\n",
        "\n",
        "3) Xử lí dữ liệu, feature engineering trên dataset đã cho:\n",
        "\n",
        "- Nhìn chung, dataset đã cho khá ổn ( đều là chữ viết thường, không chưa các dấu như “,” , “.”, …) nên ta không cần xử lí cũng được\n",
        "- Feature engineering dataset đã cho:\n",
        "+ Sử dụng CountVectorize của sklearn để chuyển chuổi sang vector với headlines tập train và test.\n",
        "+ Với tập train, ta chỉ cần sử dụng method CountVectorize().fit_transform().toarray() để convert\n",
        "+Với tập test,  vì đây là tập dữ liệu test, nó phải đồng bộ hóa với vocabulary của tập train. Nói một cách đơn giản là nếu như ta không đồng bộ hóa tập test với vocabulary tập train thì nó sẽ có thể không cùng 1 kích thước để có thể predict được. Do đó ta phải xây dựng method CountVectorize() cho riêng tập test dựa trên vocabulary của tập train bằng cách tạo 1 variable countvectorize_test mới với cú pháp như sau: countvectorize_test = CountVectorize(vocabulary=countvectorize_train.get_feature_name()), rồi sau đó convert tập test sang array như bình thường bằng lệnh: countvectorize_test().fit_transform().toarray()\n",
        "\n",
        "4) Mô tả quá trình chọn model, học\n",
        "\n",
        "- Nhóm đã thử qua một vài classification model như LogisticRegression, KNeighborsClassifier, DecisionTreeClassifier, LinearSVC thì KNeighborsClassifier và DecisionTreeClassifier thì bị tràn RAM trên google colab, còn LogisticRegression và LinearSVC thì chạy được và đạt kết quả gần như nhau nên nhóm sẽ demo cả 2 model\n",
        "- Qúa trình traing sử dụng method cho sẵn của thư viện sklearn là :\n",
        "Model = LogisticRegression()\n",
        "Model.fit(X_train, Y_train)\n",
        "\n",
        "5) Mô tả cách dùng model đã train để viết một đoạn chương trình ngắn, thực hiện sacarsm detection cho một headline bất kỳ được nhập vào\n",
        "\n",
        "Được thực hiện trong phần code\n",
        "\n",
        "6)  Đánh giá:\n",
        "- Ta dễ dàng nhận thấy rằng cho dù là dùng LinearRegression hay LinearSVC thì trên test_data cũ đều ra kết quả tốt hơn so với test_data mới. Bởi vì test_data cũ nó có những nội dung gần với train_data hơn test_data mới\n",
        "- LinearRegression hay LinearSVC đều cho ra 2 kết quả gần như nhau\n",
        "=> Bài toán này phụ thuộc rất nhiều vào train_data, nếu train_data trải đều trên các năm thì ta sẽ được một kết quả tốt hơn. Một ví dụ rõ ràng trong test của nhóm đó là: Donald Trump is st*pi* và Donald Trump is not st*pi* (để dấu * vì không dám ghi ra ) thì kết quả đều in ra là isn’t sarcastic. Vì đơn giản, train_data không chứa vocabulary “Donald Trump” \n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}